あんまり重要でないファイル達、アノテーション用にtxtをcsvにしたり、idとフード名のディクショナリーを作ったりするファイルなど。
なので適宜書き換えながら使えそうなものは使ってもらう感じで。

＃このファイルだけ少し重要。
・extractSpecificSentences.py
見出し名、ヘッダーの種類、パラグラフ、センテンス、それぞれのidに応じて文章を抽出する。
新しいフィルタリングのような扱い、このファイルをいろいろと改変して、それぞれの条件でどれぐらい文が取れて、どれぐらいエラーが増えるかなどを議論するのもありかもしれない。
<h1>と<概要>の全文、他の見出しの各パラグラフの一行目をスクレイピングして90~95%ぐらい使える(日本語として違和感のない)文章を取れた、フィルタリングが弱いものなので、これを強いものにすればほぼ95%~に近い確率で違和感のない文章が得られる可能性がある。

・document2training_data.py
話し言葉コーパスをword2vec用のトレーニングデータに書き換えるファイル

・makeCSVfile4annotation.py
txtの文章群ファイルをアノテーション用にcsvに書き換えるファイル

・REpractice.py
正規表現の練習など。

・easyScraping.py
一番単純なスクレイピング

・makeId2foodDict.py
id -> food のディクショナリー

・showCandidateSentences.py
料理名をコンソールに入れるとそれに対応した文章を出力するもの。

・extractAnnotatedSentences.py
アノテーション用のcsvからアノテーションされた文章だけを抽出するファイル

・makeNewFoodList.py
もらった２つのフードリストからページが存在するトピックだけを抜き出して新しいフードリストを作ったファイル。これで作ったフードリストを使うことで、スクレイピング時間を少しだけ短縮可能？

・analyzeDataset.py
文章群の長さなどの平均値、標準偏差などを計算、長さ６０文字で約2σ分の長さになるので、約９０％以上はカバーできている推定、なので分類でも６０文字で区切った。文章の後の方にはそこまで重要な情報はない可能性が高いので、短くすることも可能、実験すべきだったかも。

・extractHeaders.py
どんな見出しがどれぐらいの割合で含まれているかなどを確認するために、wikiページデータから見出しのみを抽出。

・makeTrainingDataFromCSV.py
アノテーションされたデータを分かち書きして分類機に渡せる状態に変換するためのファイル。

・tfidf.py
tfidfを計算するファイル。

・combine2documents.py
word2vecトレーニング用に、散らばったテキストファイルを一つにまとめるためのファイル？

・extractSentences4annotation.py
各スクレイピングしてきたファイルから１文または5文ずつアノテーション用に抜き出すファイル。正規分布を使って、ファイルの真ん中らへんの文章を抽出する確率を上げてある。

・corpus2dataset.py
話し言葉コーパスをword2vecに渡す形式に変えるファイル。

・modifyCSV.py
李さんに頼まれた、使える文章をエクセルに移すためのcsvファイルを書き換えるファイル？